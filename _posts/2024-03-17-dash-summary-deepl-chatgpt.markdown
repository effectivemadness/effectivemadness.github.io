---
title:  "귀차니스트가 외국 컨퍼런스 세션 리스트 긁고 번역 요약하는 방법(feat. DeepL, OpenAI)"
date:   2024-03-3 23:33:31+0900
categories: [Trivia]
tags: [tech, AI, chatGPT, datadog]
---
업무 중 사용하는 Datadog 에서 개최하는 컨퍼런스인 Dash에서 어떤 세션이 있는지 조사하고 볼만한게 있는지 찾아보려 했다. 단순히 세션 뭐있나 보고 정리하면 될 줄 알았는데, DeepL API와 ChatGPT를 써 보는 등 생각보다 좀 재미있는 딴짓이어서 다시 코드를 구성하며 기록으로 남겨보기로 했다.

---

## 페이지 긁어오기

크롤링 자체는 selenium을 사용해 긁어오는것이 어렵지는 않았다. 전체 세션 리스트에서 각 세션 상세 페이지 링크의 xpath를 찾고, 상세 페이지 path들을 돌면서 세션들의 Description 과 장소 정보를 긁었다. 공개된 컨퍼런스라 로그인이나 인증 등의 처리는 없었고, 단순히 경로들을 반복하면서 값을 Pandas Dataframe에 적재하는 과정이 대부분이었다.

![Session List](/assets/img/04032CD9-4860-49EE-982E-D7286D2C35EC.png)

![Session Description](/assets/img/127E239B-CCE9-400A-9D08-4CB76185EF59.png)

수집을 하고 난 뒤, 약 70개가 살짝 넘는 세션들의 정보와 Description이 손 안에 들어왔다. 해당 컨퍼런스는 모두 영어로 진행이 되고, 소개 및 세션 상세 페이지 역시 모두 영어로 작성이 되어있었다. 모든 컨퍼런스의 정보들을 한데 모을 수 있게 되자, 조금 더 편하게 이 정보들을 보고 싶어졌다.

![first crawl](/assets/img/04930862-47D8-43E1-9A9F-C32E9085EA44.png)

## 번역하기

그 뒤로, 번역 API를 찾기 시작했다. 하지만 처음부터 다른 번역 서비스가 아닌 DeepL을 타겟으로 잡았다. 개인적으로 DeepL을 사용하고 있는데, Google 번역이나 파파고보다 더 만족스러운 결과를 보여줘 아주 만족도가 높았다. 다만, DeepL API의 경우 당시 한국에서 지원을 하지 않는다는 벽을 만났다. 이후 확인해보니 2023년 9월부터 한국에서 지원을 하고 있다고 한다.

개인적으로 아주 잘 사용하고 있던 DeepL이었기에, 아쉬워 그 주변을 서성거리면서 다른 방법이 없나 찾아보는 중 rapidapi라는 API Hub 서비스를 발견했다. 자세히 확인해 보니 다른 곳에서 유료나 무료로 제공하는 API들을 한데 모아 제공하는 서비스인 rapidapi에서, DeepL API를 사용할 수 있었다. 심지어 어느 정도의 무료 사용량을 제공해주어, 쾌재를 부르며 해당 서비스에 가입해 DeepL API를 테스트해봤고 실제로 잘 작동하는것을 확인했다.

API를 사용하는것 자체는 아주 쉬웠고, 수집한 세션 Description 텍스트들을 모두 한국어로 번역할 수 있었다.

![translate to korean](/assets/img/81965924-5177-4842-9BC2-E9E03E05D414.png)

## 요약하기

서있으면 앉고싶고, 앉으면 눕고싶고, 누우면 자고싶은게 인간의 본성이라 했었나. 이제 번역까지 되었음에도 이걸 다 일일히 읽는 것 보다 더 나은 방법이 없을까? 라는 생각이 들었다. 그 때, 관심 있게 보던 ChatGPT 프롬프트 엔지니어링이 생각났다. API를 사용하면, “System” Role로 ChatGPT 모델에게 사용자의 입력을 어떻게 처리해 어떻게 응답을 줄 지 정해줄 수 있다. 당시 코드를 던져주면 변수나 함수 이름을 지어준다거나, 응답을 JSON으로 보내주게 구성하는 등 신기한 프롬프트를 많이 봤었기에 응용해보도록 했다.

OpenAI API를 사용하는 것은 어렵지는 않았다. 직접 API를 호출할 필요도 없고, Python에서 openai 패키지가 이미 있어 API 키를 넣어주고 ChatCompletion.crate() 를 통해 값을 받아오기만 하면 되었다.

```python
response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": '''You are an summarization helper. Your job is to summarize user input within 2~3 sentences.'''},
        {"role": "user", "content": "기술 업계 전반의 조직은 회사의 성장과 혁신을 지원하는 다양성, 형평성, 포용성(DEI) 프로그램을 육성하기 위해 노력하고 있습니다. 이러한 노력도 중요하지만, 일반적으로 기술 분야에서 소외된 사람들에게 일상적으로 가장 큰 영향을 미치는 것은 소규모의 팀 역학입니다. 이 패널에서는 엔지니어링 관리자와 영향력 있는 개인 기여자로부터 그들이 직면한 과제, 팀을 더욱 개방적이고 포용적이며 지원적인 조직으로 만들기 위해 개발한 솔루션, 팀 차원의 DEI 작업을 통해 얻은 긍정적인 성과에 대해 들어봅니다."}
    ]
)

# 응답: 기술 업계에서는 DEI 프로그램을 육성하고 있지만, 소규모 팀 역학이 소외된 사람들에게 큰 영향을 미칩니다. 이 패널에서는 엔지니어링 관리자와 개인 기여자의 과제, 조직의 포용적인 솔루션, 팀 DEI 작업의 긍정적인 성과에 대해 알아봅니다.
```

`system` role을 지정하면서, 유저 요청을 2~3 문장으로 요약해 달라고만 하면, 응답과 같이 깔끔하게 요약해 보내줬다. 생각보다 잘 되어서 아주 만족스럽게 모든 세션에 대해 요약을 진행했다.

여기까지만 해도 처리한 것을 10분정도 슥 보면서 정리하면 목표한 바를 이루는 것이었다. 하지만 인간의 욕심은 끝이 없다. 키워드 단위로 더 요약해본다.

```python
response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": '''This is request from automated system. you must generate answer in given form like below, in json format without comment:

```json
{
    "Summary": "summary", // String, Summarized user input within 2~3 sentences
    "Tag": ["tag1", "tag2", "tag3"] // List of String, Subject and Keyword of user input
}
```},
        {"role": "user", "content": "기술 업계 전반의 조직은 회사의 성장과 혁신을 지원하는 다양성, 형평성, 포용성(DEI) 프로그램을 육성하기 위해 노력하고 있습니다. 이러한 노력도 중요하지만, 일반적으로 기술 분야에서 소외된 사람들에게 일상적으로 가장 큰 영향을 미치는 것은 소규모의 팀 역학입니다. 이 패널에서는 엔지니어링 관리자와 영향력 있는 개인 기여자로부터 그들이 직면한 과제, 팀을 더욱 개방적이고 포용적이며 지원적인 조직으로 만들기 위해 개발한 솔루션, 팀 차원의 DEI 작업을 통해 얻은 긍정적인 성과에 대해 들어봅니다."}
    ]
)

# 응답: "Summary": "요약 내용 - 위 예시와 상동", "Tag": ['기술 업계', '다양성', '형평성', '포용성', 'DEI 프로그램', '팀 역학', '엔지니어링 관리자', '개인 기여자']
```

요약을 한 뒤 키워드를”Tag” 라는 필드에 array로 달라고 요청했다. 여기에 더해 주고받는 값이 늘면서, 단순히 스트링으로 주지 말고 JSON형태의 구조화된 응답으로 받아 처리할 때 더 편하게 처리할 수 있었다. 다른 프롬프트를 봤을 때도 놀라웠던 게, JSON 형태로 응답을 달라고 하면 그 형태로 응답을 해 후처리를 최소화 할 수 있다.

그렇게 모든 세션들의 요약 및 키워드까지 정리해 팀 구성원들에게 공유하였고, 조금 더 편하게 세션들을 정리할 수 있었다.

![final summary](../assets/img/08E72519-C9A0-4578-90C8-FA553EF228C2.png)
---

처음엔 단순히 값들만 긁어서 한 페이지에서 보는 목표를 가지고 시작했지만, DeepL 이나 OpenAI API등 평상시 써보고 싶었던 것들을 써볼 수 있어 좋았다. 당시에는 ChatGPT 4가 없어 크롤링 등 귀찮은 것들을 직접 하긴 했지만, 요즘에는 크롤링 등을 알아서 처리해준다고 한다. 기회가 된다면 써 보고 싶다. 친구 쓰는거 보니 http 메소드를 지정해(GET, POST…) 받은 값을 처리해 준다거나, 파라메터를 바꿔가며 크롤링해준다거나 하는 것들을 자연어로 처리할 수 있던데, 기술의 발전은 끝이 없는 것 같다.